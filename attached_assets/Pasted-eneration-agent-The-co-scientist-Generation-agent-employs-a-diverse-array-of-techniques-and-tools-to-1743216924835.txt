eneration agent
The co-scientist Generation agent employs a diverse array of techniques and tools to generate novel hypotheses,
such as the following:
• Literature exploration via web search. The agent iteratively searches the web, retrieves and reads
relevant research articles, and grounds its reasoning by summarizing prior work. It then builds on this
summary to generate novel hypotheses and research plans. An example prompt is given in Appendix
Figure A.24.
• Simulated scientific debates. Here, the Generation agent simulates scientific debates among experts
by employing self-critique and self-play techniques. These debates typically involve multiple turns of
conversations leading to a refined hypothesis generated at the end. An example prompt is given in
Appendix Figure A.25.
• Iterative assumptions identification. The agent iteratively identifies testable intermediate assump-
tions, which, if proven true, can lead to novel scientific discovery. These plausible assumptions and their
sub-assumptions are identified through conditional reasoning hops and subsequently aggregated into
complete hypotheses.
• Research expansion. To identify previously unexplored areas of the hypothesis space, the Generation
agent reviews existing hypotheses and the research overview and feedback provided by the Meta-review
agent in the previous iteration. This is used to inform additional exploration directions in the research
hypothesis space.
An example hypothesis and research proposal output from the Generation agent is presented in Appendix
Figure A.2 for the aforementioned research goal regarding explaining a basic mechanism related to ALS. The
Generation agent also summarizes and categorizes each generated hypothesis, allowing scientists to quickly
grasp the core ideas.
3.3.2 Reflection agent
Reviews are integral to the co-scientist’s effectiveness in generating novel proposals. The Reflection agent
searches relevant prior work (via web search or a dedicated scientist-provided repository), assesses existing
experimental evidence for or against a given hypothesis, and rigorously verifies the novelty, correctness, and
quality of generated outputs. Effective reviews filter inaccurate and, when stipulated, non-novel hypotheses.
Moreover, they also provide feedback to all other agents, driving continuous improvement. The Reflection
agent employs the following types of review:
• Initial review. Building on the co-scientist’s default evaluation criteria, the Reflection agent performs
an initial review assessing the correctness, quality, novelty, and a preliminary assessment of safety (ethics)
of the generated hypotheses. For a more in-depth discussion on safety considerations see Section 6. This
initial review, which doesn’t use external tools like web search, aims to quickly discard flawed, non-novel,
or otherwise unsuitable hypotheses.
• Full review. If a hypothesis passes the initial review, the Reflection agent performs a full review,
leveraging external tools and web searches to identify relevant articles for improved reasoning and
grounding. This review evaluates the hypothesis’s correctness, quality, and novelty similar to the initial
review but with full literature search. For correctness and quality, the agent scrutinizes underlying
assumptions and reasoning. For novelty, it summarizes known aspects of the hypothesis and then
judges their novelty based on existing literature. An example full novelty review is shown in Appendix
Figure A.3, and an example of review critiques is in Appendix Figure A.4. A complete full review
example is shown in Appendix Figure A.5.
• Deep verification review. The Reflection agent also conducts a deep verification review, decomposing
the hypothesis into constituent assumptions. Each assumption is further broken down into fundamental
sub-assumptions, decontextualized, and independently evaluated for correctness to identify invalidating
elements for subsequent filtering. Concurrently, the reasons for potential hypothesis invalidation due
to incorrect assumptions are summarized. This deep verification helps the co-scientist detect subtle
errors within complex hypotheses, such as flaws in reasoning or inaccurate experimental protocols. An
|10
identified error doesn’t necessarily invalidate the core hypothesis; the Reflection agent assesses whether
the incorrect assumption is fundamental to the hypothesis and incorporates this reasoning into the
review. Non-fundamental errors can be addressed during subsequent refinement stages. An example deep
verification review is provided in Appendix Figure A.6 for the previously introduced ALS hypothesis.
We also show another example of a deep verification review via probing questions in the context of drug
repurposing for AML in Appendix Figure A.7.
• Observation review. In addition, the Reflection agent also explores whether a given hypothesis can
account for long-tail observations from prior experimental results. This review aims to determine if
the hypothesis can provide insights on existing experimental findings and observed phenomena within
relevant articles. For each observation, the agent assesses if the hypothesis is a superior explanation
over existing ones, assuming its validity. Positive observations are summarized and appended to the
hypothesis. Note that this review often completes without any important findings (as in the case of
the ALS hypothesis example). An example prompt to generate observations is provided in Appendix
Figure A.26. An illustrative example of an observation review is provided in Appendix Figure A.8 in
the context of an alternate hypothesis for explaining a mechanism of anti-microbial resistance.
• Simulation review. The Reflection agent also reviews hypotheses by simulating them in a step-wise
fashion (e.g., simulating the mechanism of action or the proposed experiment in the proposal). This
simulation allows the agent to identify and summarize potential failure scenarios. This review method is
built on the assumption that frontier LLMs may have developed an internal world model that enables
them to simulate and accurately predict various scientific phenomena.
• Recurrent/tournament review. The Reflection agent adapts its full reviews based on the co-
scientist’s growing knowledge. By analyzing reviewed hypotheses and results of the tournament
conducted by the Ranking agent, the Reflection agent identifies recurring issues and improvement
opportunities, refining its reviews accordingly.
Additionally, the co-scientist can incorporate reviews from expert scientists to guide ranking and system
improvements (further discussed in Section 3.4). We aim to have the Reflection agent’s comprehensive set of
reviews cover the common methods scientists employ when critiquing and refining research hypotheses and
proposals.
3.3.3 Ranking agent
The AI co-scientist explores numerous hypotheses and research proposals towards a research goal, necessitating
a ranking mechanism to prioritize computational resources toward the most promising candidates. This task
is performed by the Ranking agent. The agent uses an Elo-based tournament [61] to automatically evaluate
and rank all hypotheses, providing supporting rationale. This ranking serves to communicate to scientists
an ordered list of research hypotheses and proposals aligned with the research goal. Despite its assumptions
and limitations [63], Elo remains a good proxy for relative ranking, and it has previously been applied to
rank extracted patterns and ideas in games [64]. In the future, extensions may be considered [65]. We set the
initial Elo rating of 1200 for the newly added hypothesis.
Because the tournament is computationally intensive, the Ranking agent employs several optimization
strategies. Top-ranked hypotheses are compared pairwise in tournament matches through multi-turn scientific
debates [66]. This mitigates ordering bias and focuses on novelty, correctness, and testability. Lower-ranked
hypotheses undergo single-turn comparisons in a pairwise fashion in their tournament match. The agent
concludes each comparison with a decision regarding which hypothesis is better. Appendix Figure A.27 and
Appendix Figure A.28 show example prompts. Appendix Figure A.9 shows an example of the Ranking agent
conducting a scientific debate match in a tournament to compare two hypotheses.
The Ranking agent prioritizes tournament matches as follows: (1) hypotheses are more likely to be compared
with similar ones (based on the Proximity agent’s graph, described in the next section); (2) newer and
top-ranking hypotheses are prioritized for participation in tournament matches. Successful hypotheses quickly
achieve favorable rankings and this informs the tournament state for subsequent iterations.
|11
3.3.4 Proximity agent
The Proximity agent calculates the similarity between research hypotheses and proposals, and builds a
proximity graph, taking into account the specific research goal. Although it doesn’t directly participate in
hypothesis generation, the Proximity agent assists the Ranking agent in organizing tournament matches and
showcasing a diverse range of ideas related to the research goal. This allows scientists to quickly explore areas
of interest and easily identify related concepts.
3.3.5 Evolution agent
The Evolution agent continuously refines and improves existing hypotheses and proposals using several
approaches including:
• Enhancement through grounding. Here the agent attempts to improve hypotheses by identifying
weaknesses, generating search queries, retrieving and reading articles, suggesting improvements and
elaborating on details to fill reasoning gaps.
• Coherence, practicality and feasibility improvements. The agent aims to address issues and
creates more coherent hypotheses, potentially rectifying underlying problems with invalid initial as-
sumptions. The agent also refines the hypotheses to make them more practical and feasible. Appendix
Figure A.29 provides an example of the feasibility improvement prompt.
• Inspiration from existing hypotheses. The agent additionally creates new hypotheses inspired by
single or multiple top-ranked hypotheses.
• Combination. The agent also attempts to directly combine the best aspects of several top-ranking
hypotheses to create new hypotheses.
• Simplification. The agent simplifies hypotheses for easier verification and testing.
• Out-of-box thinking. The agent also explores out-of-the-box ideas by moving away from a subset of
hypotheses and generating divergent ones. Appendix Figure A.30 provides an example prompt for this.
The Evolution agent generates new hypotheses; it doesn’t modify or replace existing ones. This strategy
protects the quality of top-ranked hypotheses from flawed improvements, as each new hypothesis must also
compete in the tournament. The evolution of research hypotheses and proposals also allows the co-scientist to
iteratively combine different improvement techniques and gradually improve the quality of the results.
3.3.6 Meta-review agent
The Meta-review agent plays a crucial role in the co-scientist’s feedback loop, enabling self-improvement in
scientific reasoning. This agent operates on the tournament state and summarizes common patterns identified
in reviews and scientific debates in the tournament matches into a meta-review critique.
By synthesizing insights from all reviews, the meta-review provides valuable feedback to the Reflection agent,
leading to more thorough and reliable future reviews. This helps prevent oversight of critical details. Consider
the illustrative example of a identifying a repurposing drug candidate for ALS as a research goal: while only
90% of individual reviews might correctly identify a blood-brain barrier permeability issue in a proposed
candidate, the meta-review ensures that all future reviews by the Reflection Agent definitively address this
crucial factor. Hypothesis and research proposal generation is also enhanced by the meta-review’s identification
of recurring issues. While the Generation agent uses this feedback selectively to avoid over fitting to these
review critiques, it helps prevent the recurrence of common issues.
Appendix Figure A.31 provides an example prompt for the meta-review. In Appendix Figure A.10-A.11, we
showcase an example of the summarized meta-review critique generated for the reviews of the previously
introduced ALS mechanism hypotheses.
Research overview generation. The Meta-review agent periodically synthesizes top-ranked hypotheses into
a research overview, providing a roadmap for future research. This overview outlines potential research areas
and directions relevant to the research goal, justifying their importance and suggesting specific experiments
within each. Each area includes illustrative example topics. The research overview also serves as an additional
input to the Generation agent in subsequent iterations.
|12
The research overview serves to effectively map the boundary of current knowledge relevant to the research
goal in the co-scientist system and helps highlight future areas of exploration. In Appendix Figure A.12-A.13,
we show an example of a research overview for the ALS mechanism research goal.
The Meta-review agent can further format these overviews using constrained decoding techniques [67] to
adhere to common research publication and grant formats (e.g., National Institute of Health (NIH) Specific
Aims Page format). We demonstrate the effectiveness of this in subsequent sections